{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Classification\n",
    "\n",
    "\"\"\"\n",
    "seed = 42\n",
    "random_state= seed\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed()\n",
    "import random\n",
    "random.seed()\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import pickle as pkl\n",
    "import time\n",
    "import pandas as pd\n",
    "# import scipy\n",
    "# import shap\n",
    "import math\n",
    "import sklearn\n",
    "import scipy.stats as st\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "# from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.under_sampling import RandomUnderSampler,TomekLinks,NeighbourhoodCleaningRule,NearMiss\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve,auc, classification_report, precision_score, confusion_matrix, recall_score\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# from scipy import interp\n",
    "# import category_encoders as ce\n",
    "# from fancyimpute import KNN\n",
    "# from fancyimpute import IterativeImputer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pickle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ElasticNet_function(model_name, hyperparams,X,y,y_flag_positive):\n",
    "    X = np.array(X)\n",
    "    y = y\n",
    "    y_flag_positive = np.array(y_flag_positive)\n",
    "\n",
    "    # Assign hyper-parameters\n",
    "    alpha_value, l1_ratio_value = hyperparams\n",
    "    # Initialize lists with final results\n",
    "    y_pred_total = []\n",
    "    y_test_total = []\n",
    "    # Split data into test and train: random state fixed for reproducibility\n",
    "    kf = KFold(n_splits=5,shuffle=True,random_state=42)\n",
    "    # kf-fold cross-validation loop\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # y_flag_positive_train, y_flag_positive_test = y_flag_positive[train_index], y_flag_positive[test_index]\n",
    "\n",
    "        # # only select positive samples for testing\n",
    "        # positive_index = np.argwhere(y_flag_positive_test == 1).flatten()\n",
    "        # X_test = X_test[positive_index]\n",
    "        # y_test = y_test[positive_index]\n",
    "\n",
    "        # # Scale X_train and X_test\n",
    "        # scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "        # X_train_scaled = scaler.transform(X_train)\n",
    "        # X_test_scaled = scaler.transform(X_test)\n",
    "        X_train_scaled = X_train\n",
    "        X_test_scaled = X_test\n",
    "        # Fit KRR with (X_train_scaled, y_train), and predict X_test_scaled\n",
    "        if model_name =='elasticNet':\n",
    "            KRR = ElasticNet(random_state=seed,alpha=alpha_value,l1_ratio=l1_ratio_value) #LinearRegression()\n",
    "            model_s = KRR.fit(X_train_scaled, y_train)\n",
    "        # y_pred = KRR.fit(X_train_scaled, y_train).predict(X_test_scaled)\n",
    "        y_pred = model_s.predict(X_test_scaled)\n",
    "        # Append y_pred and y_test values of this k-fold step to list with total values\n",
    "        y_pred_total.append(y_pred)\n",
    "        y_test_total.append(y_test)\n",
    "    # Flatten lists with test and predicted values\n",
    "    y_pred_total = [item for sublist in y_pred_total for item in sublist]\n",
    "    y_test_total = [item for sublist in y_test_total for item in sublist]\n",
    "    # Calculate error metric of test and predicted values: rmse\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_total, y_pred_total))\n",
    "    # r_pearson,_=pearsonr(y_test_total,y_pred_total)\n",
    "    # print('KRR k-fold cross-validation . alpha: %7.6f, gamma: %7.4f, RMSE: %7.4f, r: %7.4f' %(alpha_value,gamma_value,rmse,r_pearson))\n",
    "    print('KRR k-fold cross-validation . alpha: %7.6f, l1_ratio_value: %7.4f, RMSE: %7.4f' % (alpha_value, l1_ratio_value, rmse))\n",
    "    return rmse, model_s\n",
    "\n",
    "def create_hyperparams_grid(X,y,y_flag_positive,model_name,params):\n",
    "    graph_x = []\n",
    "    graph_y = []\n",
    "    graph_z = []\n",
    "    graph_model_s = []\n",
    "    alpha_value_s = params['alpha']\n",
    "    l1_ratio_value_s = params['l1_ratio']\n",
    "\n",
    "    for alpha_value in alpha_value_s:\n",
    "        # alpha_value = pow(10,alpha_value)\n",
    "        graph_x_row = []\n",
    "        graph_y_row = []\n",
    "        graph_z_row = []\n",
    "        graph_model_s_row = []\n",
    "        for l1_ratio_value in l1_ratio_value_s:\n",
    "            hyperparams = (alpha_value,l1_ratio_value)\n",
    "            rmse,model_s = ElasticNet_function(model_name, hyperparams, X, y, y_flag_positive)\n",
    "            graph_x_row.append(alpha_value)\n",
    "            graph_y_row.append(l1_ratio_value)\n",
    "            graph_z_row.append(rmse)\n",
    "            graph_model_s_row.append(model_s)\n",
    "        graph_x.append(graph_x_row)\n",
    "        graph_y.append(graph_y_row)\n",
    "        graph_z.append(graph_z_row)\n",
    "        graph_model_s.append(graph_model_s_row)\n",
    "        print('')\n",
    "    graph_x=np.array(graph_x)\n",
    "    graph_y=np.array(graph_y)\n",
    "    graph_z=np.array(graph_z)\n",
    "    graph_model_s = np.array(graph_model_s)\n",
    "    min_z = np.min(graph_z)\n",
    "    pos_min_z = np.argwhere(graph_z == np.min(graph_z))[0]\n",
    "    print('Minimum RMSE: %.4f' %(min_z))\n",
    "    print('Optimum alpha: %f' %(graph_x[pos_min_z[0],pos_min_z[1]]))\n",
    "    print('Optimum l1_ratio: %f' %(graph_y[pos_min_z[0],pos_min_z[1]]))\n",
    "    best_alpha = graph_x[pos_min_z[0],pos_min_z[1]]\n",
    "    best_l1_ratio = graph_y[pos_min_z[0],pos_min_z[1]]\n",
    "    best_model = graph_model_s[pos_min_z[0],pos_min_z[1]]\n",
    "    best_rmse = min_z\n",
    "\n",
    "    return best_alpha, best_l1_ratio, best_model, best_rmse\n",
    "\n",
    "def Build_GridSearch(model_name, params, X_train, y_proba_train, y_train):\n",
    "    X = X_train.copy()\n",
    "    y = y_proba_train.copy()\n",
    "    y_train_flag_positive = y_train.copy()\n",
    "    best_alpha, best_l1_ratio, best_model, best_rmse = create_hyperparams_grid(X, y, y_train_flag_positive, model_name, params)\n",
    "\n",
    "    return best_alpha, best_l1_ratio, best_model, best_rmse\n",
    "\n",
    "def build_classifier_0(X_train,y_train,X_test,y_test,feature_cols,classifier_flag, n_fold,file_path): #X[train], y[train],X[test]\n",
    "# change label    so as to the objective case is \"1\", the objective of change label is to keep  classifier.predict_proba(X_text)[:, 1]   \"1\" is case\n",
    "    print('---------------------this is classifier_0----------------------')\n",
    "    print('y_train_number_before_change_label',y_train.value_counts())\n",
    "    # first replace \"0\" with 4, then replace 1,2,3, with \"0\", then replace 4 with \"1\"\n",
    "    y_train = y_train.replace(0, 4)\n",
    "    y_test = y_test.replace(0, 4)\n",
    "\n",
    "    # then replace 1,2,3, with \"0\",\n",
    "    y_train = y_train.replace(1, 0)\n",
    "    y_train = y_train.replace(2, 0)\n",
    "    y_train = y_train.replace(3, 0)\n",
    "\n",
    "    y_test = y_test.replace(1, 0)\n",
    "    y_test = y_test.replace(2, 0)\n",
    "    y_test = y_test.replace(3, 0)\n",
    "\n",
    "    # then replace 4 with \"1\"\n",
    "    y_train = y_train.replace(4, 1)\n",
    "    y_test = y_test.replace(4, 1)\n",
    "\n",
    "    # print('y_train', y_train)\n",
    "    print('y_train_number_after_change_label', y_train.value_counts())\n",
    "\n",
    "# not address imbalance\n",
    "#     X_train_imbalanced, y_train_imbalanced = X_train, np.array(y_train)\n",
    "\n",
    "# address imbalance\n",
    "    cc = SMOTETomek(random_state=42) # ,sampling_strategy={1:num_control}\n",
    "    # cc = RandomOverSampler(random_state=42)\n",
    "    # cc = ClusterCentroids(random_state=42)\n",
    "#\n",
    "    X_train_imbalanced, y_train_imbalanced = cc.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "# load the classifier\n",
    "#     with open(file_path + classifier_flag + '_cv_fold_' + str(n_fold) + '_classifier.pkl', 'rb') as fid:\n",
    "#         classifier = pkl.load(fid)\n",
    "\n",
    "# GridSearchCV\n",
    "    classifier = RandomForestClassifier(random_state=seed)\n",
    "    params = {'n_estimators': list(range(10, 20, 10)),  # list(range(20,120,10)),\n",
    "              'max_depth': list(range(2, 17, 2)),\n",
    "              # 'criterion': ['gini', 'entropy'],\n",
    "              #  'min_samples_split': list(range(2,10,2)),\n",
    "              # 'class_weight': ['balanced'],  # , None\n",
    "              # 'max_leaf_nodes': list(range(2,20,2)),\n",
    "              # 'max_features': list(range(2,20,2)),\n",
    "              # 'min_samples_leaf': list(range(2,20,2))\n",
    "              }\n",
    "    grid = GridSearchCV(classifier, param_grid=params, scoring='roc_auc', cv=5, n_jobs=4)\n",
    "    grid.fit(X_train_imbalanced, y_train_imbalanced)\n",
    "    scores = pd.DataFrame(grid.cv_results_)\n",
    "    # print('scores',scores)\n",
    "    grid_accs = scores['mean_test_score'].values.tolist()\n",
    "    best_auc = grid.best_score_\n",
    "    best_auc_std = scores.loc[grid.best_index_, 'std_test_score']\n",
    "    # print(best_auc_std)\n",
    "\n",
    "    classifier = grid.best_estimator_\n",
    "#\n",
    "    print('Best by searching: %s, Std: %s' % (best_auc, best_auc_std))\n",
    "    best_train_auc= best_auc\n",
    "    print(grid.best_params_)\n",
    "\n",
    "# training classifier\n",
    "#     classifier = RandomForestClassifier(random_state =seed,n_estimators = 100,max_depth =8,class_weight ='balanced')\n",
    "    classifier.fit(X_train_imbalanced, y_train_imbalanced)\n",
    "\n",
    "# save the classifier\n",
    "    with open(file_path+classifier_flag+'_cv_fold_'+str(n_fold)+'_classifier.pkl', 'wb') as fid:\n",
    "        pkl.dump(classifier, fid)\n",
    "\n",
    "    y_proba = classifier.predict_proba(X_test)[:, 1]  #\n",
    "    # y_predic = classifier.predict(X_test)\n",
    "    # print('y_predic',y_predic)\n",
    "\n",
    "    test_result = y_proba\n",
    "    aucroc = roc_auc_score(y_test, y_proba)\n",
    "    print('aucroc', aucroc)\n",
    "\n",
    "    # precision-recall auc\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "    aucpr = auc(recall, precision)\n",
    "    print('aucpr', aucpr)\n",
    "\n",
    "#  plot feature importance\n",
    "    X_data = X_train_imbalanced\n",
    "    feature_cols = feature_cols\n",
    "\n",
    "    # fig, ax = plt.subplots(figsize=(15, 12))\n",
    "    # fig.subplots_adjust(left=0.2, right=0.95, top=0.95, bottom=0.08, hspace=.5, wspace=.15)\n",
    "    #\n",
    "    importances = classifier.feature_importances_\n",
    "    # # print('importances',importances)\n",
    "    num_features = len(importances)\n",
    "    # indices = np.argsort(abs(importances))[::-1]\n",
    "    # plt.title(\"Feature importance\")\n",
    "    # plt.barh(list(range(X_data.shape[1]))[:num_features], list(importances[indices])[:num_features], align=\"center\")\n",
    "    # plt.yticks(list(range(X_data.shape[1]))[:num_features], (np.array(feature_cols)[indices]).tolist()[:num_features])\n",
    "    #\n",
    "    # # plt.show()\n",
    "    # fig_name = file_path+classifier_flag+'_cv_fold_'+str(n_fold)+'_feature_importance_aucroc'+str(aucroc)+'_aucpr_'+str(aucpr)+'.png'\n",
    "    # fig.savefig(fig_name)\n",
    "    # plt.close('all')\n",
    "\n",
    "    # print(importances.shape)\n",
    "    importances_df = pd.DataFrame(importances.reshape((1,-1)),columns=feature_cols)\n",
    "    importances_df.to_csv(file_path+classifier_flag+'_cv_fold_'+str(n_fold)+'_feature_importance_aucroc'+str(aucroc)+'_aucpr_'+str(aucpr)+'.csv',index=False,header=True)\n",
    "\n",
    "    stu_model = DecisionTreeRegressor(max_depth=5)\n",
    "    y_proba_train = classifier.predict_proba(X_train)[:, 1]\n",
    "    stu_model.fit(X_train, y_proba_train)\n",
    "\n",
    "    y_proba_test_stu = stu_model.predict(X_test)\n",
    "\n",
    "    return test_result\n",
    "\n",
    "def build_classifier_1(X_train,y_train,X_test,y_test,feature_cols,classifier_flag, n_fold,file_path): #X[train], y[train],X[test]\n",
    "# change label    so as to the objective case is \"1\", the objective of change label is to keep  classifier.predict_proba(X_text)[:, 1]   \"1\" is case\n",
    "    print('---------------------this is classifier_1----------------------')\n",
    "    print('y_train_number_before_change_label',y_train.value_counts())\n",
    "    # replace 2,3 with 0,\n",
    "    y_train = y_train.replace(2, 0)\n",
    "    y_train = y_train.replace(3, 0)\n",
    "    y_test = y_test.replace(2, 0)\n",
    "    y_test = y_test.replace(3, 0)\n",
    "\n",
    "    # print('y_train', y_train)\n",
    "    print('y_train_number_after_change_label', y_train.value_counts())\n",
    "\n",
    "# GridSearchCV\n",
    "    classifier = RandomForestClassifier(random_state=seed)\n",
    "    # classifier = DecisionTreeClassifier(random_state=seed)\n",
    "    # classifier = LogisticRegression(random_state=seed,max_iter = 200)\n",
    "    #\n",
    "    # params = {\n",
    "    #         'C': [0.7],# 0.08, 0.1, 0.3, 0.5, 0.6, 1, 2, 3, 4,\n",
    "    #         # 'l1_ratio':[0.3],\n",
    "    #         'penalty':['l1'], # 'elasticnet'\n",
    "    #         'solver':['saga'] # liblinear\n",
    "    #         }\n",
    "\n",
    "    params = {\n",
    "              'n_estimators': [100], # for 24hours, worst, [110]\n",
    "              'max_depth': [6], # for 24hours, mean, [10],for 24hours, worst, [12]\n",
    "              # 'criterion': ['gini', 'entropy'],\n",
    "              #  'min_samples_split': list(range(2,10,2)),\n",
    "              'class_weight': ['balanced'],  # , None\n",
    "              # 'max_leaf_nodes': list(range(2,10,2)),\n",
    "              # 'max_features': list(range(2,20,2)),\n",
    "              # 'min_samples_leaf': list(range(2,20,2))\n",
    "              }\n",
    "\n",
    "    grid = GridSearchCV(classifier, param_grid=params, scoring='roc_auc', cv=5, n_jobs=4) # scoring='roc_auc', 'precision'\n",
    "    grid.fit(X_train, y_train)\n",
    "    scores = pd.DataFrame(grid.cv_results_)\n",
    "    # print('scores',scores)\n",
    "    grid_accs = scores['mean_test_score'].values.tolist()\n",
    "    best_acc = grid.best_score_\n",
    "    best_acc_std = scores.loc[grid.best_index_, 'std_test_score']\n",
    "    # print(best_auc_std)\n",
    "    classifier = grid.best_estimator_\n",
    "    print('Best by searching: %s, Std: %s' % (best_acc, best_acc_std))\n",
    "    print(grid.best_params_)\n",
    "\n",
    "# training classifier on all train data\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_proba = classifier.predict_proba(X_test)[:, 1]\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    test_result = y_proba\n",
    "\n",
    "    # print('y_proba',y_proba)\n",
    "    aucroc = roc_auc_score(y_test, y_proba)\n",
    "    print('aucroc', aucroc)\n",
    "\n",
    "    # precision-recall auc\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "    # print('precision',precision)\n",
    "    # print('recall',recall)\n",
    "    aucpr = auc(recall, precision)\n",
    "    print('aucpr', aucpr)\n",
    "\n",
    "    precision_RI = precision_score(y_test, y_pred)\n",
    "    recall_RI = recall_score(y_test, y_pred)\n",
    "    confusion_matrix_RI = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print('precision_RI:', precision_RI)\n",
    "    print('recall_RI:', recall_RI)\n",
    "    print('confusion_matrix_RI:', confusion_matrix_RI)\n",
    "\n",
    "    # # plot precision recall curve\n",
    "    # fig, ax = plt.subplots()\n",
    "    # ax.plot(recall, precision, color='purple')\n",
    "    #\n",
    "    # #add axis labels to plot\n",
    "    # ax.set_title('RI-Precision-Recall Curve')\n",
    "    # ax.set_ylabel('Precision')\n",
    "    # ax.set_xlabel('Recall')\n",
    "    # plt.ylim(bottom=0)\n",
    "    # #display plot\n",
    "    # # plt.show()\n",
    "    # current_time = datetime.now()\n",
    "    # fig_name_0 = \"RI_Precision_Recall_curve\"+\"_\"+str(current_time)+\".png\"\n",
    "    # fig.savefig(file_path + fig_name_0)\n",
    "    # plt.close('all')\n",
    "\n",
    "# plot decision tree\n",
    "#     fig = plt.figure(figsize=(50, 30))\n",
    "#     _ = tree.plot_tree(classifier,\n",
    "#                        feature_names=feature_cols,\n",
    "#                        class_names=[\"Not Ripidly Improving\", \"Ripidly Improving\"],\n",
    "#                        filled=True, fontsize=20)\n",
    "#\n",
    "#     fig_name = \"RI_visualization.png\"\n",
    "#     fig.savefig(file_path + fig_name)\n",
    "#     plt.close('all')\n",
    "\n",
    "# save the classifier\n",
    "    with open(file_path + 'RI_ori.pkl', 'wb') as fid:\n",
    "        pkl.dump(classifier, fid)\n",
    "\n",
    "#  plot feature importance\n",
    "    X_data = X_train\n",
    "    feature_cols = feature_cols\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 12))\n",
    "    fig.subplots_adjust(left=0.2, right=0.95, top=0.95, bottom=0.08, hspace=.5, wspace=.15)\n",
    "\n",
    "    importances = classifier.feature_importances_\n",
    "    # importances = classifier.coef_.reshape(-1)\n",
    "\n",
    "    # print('importances',importances)\n",
    "    num_features = len(importances)\n",
    "    indices = np.argsort(abs(importances))[::-1] #\n",
    "    plt.title(\"Feature importance\")\n",
    "    plt.barh(list(range(X_data.shape[1]))[:num_features], list(importances[indices])[:num_features], align=\"center\")\n",
    "    plt.yticks(list(range(X_data.shape[1]))[:num_features], (np.array(feature_cols)[indices]).tolist()[:num_features])\n",
    "\n",
    "    # plt.show()\n",
    "    fig_name = file_path+classifier_flag+'_cv_fold_'+str(n_fold)+'_feature_importance_aucroc'+str(aucroc)+'_aucpr_'+str(aucpr)+'.png'\n",
    "    fig.savefig(fig_name)\n",
    "    plt.close('all')\n",
    "\n",
    "    # print(importances.shape)\n",
    "    importances_df = pd.DataFrame(importances.reshape((1,-1)),columns=feature_cols)\n",
    "    importances_df.to_csv(file_path+classifier_flag+'_teacher'+'.csv',index=False,header=True)\n",
    "\n",
    "    if num_sel_feat > 0:\n",
    "        sel_idx = np.argsort(importances)[::-1][:num_sel_feat]\n",
    "        X_train = X_train.iloc[:, sel_idx]\n",
    "        X_test = X_test.iloc[:, sel_idx]\n",
    "\n",
    "    if distill_method == \"mimic_teacher\":\n",
    "        if distill_model == \"tree\":\n",
    "            stu_model = DecisionTreeRegressor(max_depth=4)\n",
    "        elif distill_model == \"lr\":\n",
    "            stu_model = ElasticNet(random_state=seed,normalize=True) #LinearRegression()\n",
    "            # stu_model = LogisticRegression()\n",
    "            # tune\n",
    "            params = {'alpha':[0.00003], # 0.0003, 0.00025,0.0004, 0.00005,0.00045, 0.01\n",
    "                    'l1_ratio': [1],\n",
    "                    }\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        y_proba_train = classifier.predict_proba(X_train)[:, 1]\n",
    "        y_proba_test = classifier.predict_proba(X_test)[:, 1]\n",
    "        y_pos_proba_test = classifier.predict_proba(X_test.loc[y_test[y_test == 1].index])[:, 1]\n",
    "\n",
    "        grid = GridSearchCV(stu_model, param_grid=params, scoring='neg_mean_squared_error',  cv=5, n_jobs=4)  # scoring='roc_auc', 'precision'\n",
    "        grid.fit(X_train, y_proba_train)\n",
    "        stu_model = grid.best_estimator_\n",
    "        print('Best by searching: %s, Std: %s' % (best_acc, best_acc_std))\n",
    "        print(grid.best_params_)\n",
    "\n",
    "        # # build GridSearch\n",
    "        # model_name = 'elasticNet'\n",
    "        # # only select positive samples\n",
    "        # positive_index = list(y_train[y_train == 1].index)\n",
    "        # X_train = X_train.loc[positive_index]\n",
    "        # positive_index_in_proba = np.argwhere(np.array(y_train) == 1).flatten()\n",
    "        # y_proba_train = y_proba_train[positive_index_in_proba]\n",
    "        #\n",
    "        # best_alpha,best_l1_ratio,best_model,best_rmse = Build_GridSearch(model_name, params, X_train, y_proba_train, y_train) # y_train for selecting only positive samples\n",
    "        # print('best model hyperparameters:',best_alpha, best_l1_ratio)\n",
    "        # print('best model hyperparameters----best_rmse:', best_rmse)\n",
    "        # stu_model = best_model\n",
    "\n",
    "        # training classifier on all train data\n",
    "        stu_model.fit(X_train, y_proba_train)\n",
    "        print('stu_model.coef_',stu_model.coef_)\n",
    "\n",
    "        # save the stu_model\n",
    "        with open(file_path + 'RI_simplified.pkl', 'wb') as fid:\n",
    "            pkl.dump(stu_model, fid)\n",
    "\n",
    "        # with open(file_path+'RI_simplified.pkl', 'rb') as file:\n",
    "        #     dataee = pickle.load(file)\n",
    "        # print(dataee)\n",
    "\n",
    "    elif distill_method == \"train_from_scratch\":\n",
    "        if distill_model == \"tree\":\n",
    "            stu_model = DecisionTreeClassifier(max_depth=4)\n",
    "        elif distill_model == \"lr\":\n",
    "            stu_model = LogisticRegression()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        stu_model.fit(X_train, y_train)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    print(\"For Student Model:\")\n",
    "    y_proba_test_stu = stu_model.predict(X_test) #\n",
    "\n",
    "    stu_model_rmse = mean_squared_error(y_proba_test, y_proba_test_stu, squared=False)\n",
    "    stu_model_mse = mean_squared_error(y_proba_test, y_proba_test_stu, squared=True)\n",
    "    print('stu_model_rmse',stu_model_rmse)\n",
    "    print('stu_model_mse',stu_model_mse)\n",
    "\n",
    "    # only focus on positive sample's rmse and mse\n",
    "    X_pos_test = X_test.loc[y_test[y_test == 1].index]\n",
    "    y_pos_proba_test_stu = stu_model.predict(X_pos_test)\n",
    "    stu_pos_model_rmse = mean_squared_error(y_pos_proba_test, y_pos_proba_test_stu, squared=False)\n",
    "    stu_pos_model_mse = mean_squared_error(y_pos_proba_test, y_pos_proba_test_stu, squared=True)\n",
    "    print('stu_pos_model_rmse', stu_pos_model_rmse)\n",
    "    print('stu_pos_model_mse', stu_pos_model_mse)\n",
    "\n",
    "    # compute true positive\n",
    "    RI_threshold = 0.5 # 0.65\n",
    "    rec_pos = np.count_nonzero(y_pos_proba_test_stu >= RI_threshold)/len(y_pos_proba_test_stu)\n",
    "    print('rec_pos',rec_pos)\n",
    "    y_pred_test_stu = y_proba_test_stu.copy()\n",
    "    y_pred_test_stu[y_pred_test_stu>=RI_threshold] = 1\n",
    "    y_pred_test_stu[y_pred_test_stu<RI_threshold] = 0\n",
    "\n",
    "    precision_RI_stu = precision_score(y_test, y_pred_test_stu)\n",
    "    print('precision_RI_stu:', precision_RI_stu)\n",
    "    recall_RI_stu = recall_score(y_test, y_pred_test_stu)\n",
    "    print('recall_RI_stu:', recall_RI_stu)\n",
    "    confusion_matrix_RI_stu = confusion_matrix(y_test, y_pred_test_stu)\n",
    "    print('confusion_matrix_stu:', confusion_matrix_RI_stu)\n",
    "\n",
    "    aucroc = roc_auc_score(y_test, y_proba_test_stu)\n",
    "    print('aucroc', aucroc)\n",
    "\n",
    "    # precision-recall auc\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_proba_test_stu)\n",
    "    aucpr = auc(recall, precision)\n",
    "    print('aucpr', aucpr)\n",
    "\n",
    "\n",
    "    # # # plot precision recall curve\n",
    "    # fig, ax = plt.subplots()\n",
    "    # ax.plot(recall, precision, color='purple')\n",
    "    #\n",
    "    # #add axis labels to plot\n",
    "    # ax.set_title('stu_RI-Precision-Recall Curve')\n",
    "    # ax.set_ylabel('Precision')\n",
    "    # ax.set_xlabel('Recall')\n",
    "    # plt.ylim(bottom=0)\n",
    "    # #display plot\n",
    "    # # plt.show()\n",
    "    # current_time = datetime.now()\n",
    "    # fig_name_0 = \"stu_RI_Precision_Recall_curve\"+\"_\"+str(current_time)+\".png\"\n",
    "    # fig.savefig(file_path + fig_name_0)\n",
    "    # plt.close('all')\n",
    "\n",
    "#  plot feature importance\n",
    "    X_data = X_train\n",
    "    feature_cols = feature_cols\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 12))\n",
    "    fig.subplots_adjust(left=0.2, right=0.95, top=0.95, bottom=0.08, hspace=.5, wspace=.15)\n",
    "\n",
    "    importances = stu_model.coef_\n",
    "    # importances = stu_model.coef_.reshape(-1)\n",
    "\n",
    "    # print('importances',importances)\n",
    "    num_features = len(importances)\n",
    "    indices = np.argsort(abs(importances))[::-1] #\n",
    "    plt.title(\"Feature importance\")\n",
    "    plt.barh(list(range(X_data.shape[1]))[:num_features], list(importances[indices])[:num_features], align=\"center\")\n",
    "    plt.yticks(list(range(X_data.shape[1]))[:num_features], (np.array(feature_cols)[indices]).tolist()[:num_features])\n",
    "\n",
    "    # plt.show()\n",
    "    fig_name = file_path+classifier_flag+'_student_'+'.png'\n",
    "    fig.savefig(fig_name)\n",
    "    plt.close('all')\n",
    "\n",
    "    return test_result\n",
    "    # return y_proba_test_stu\n",
    "\n",
    "\n",
    "def build_classifier_3(X_train,y_train,X_test,y_test,feature_cols,classifier_flag, n_fold,file_path): #X[train], y[train],X[test]\n",
    "# change label    so as to the objective case is \"1\", the objective of change label is to keep  classifier.predict_proba(X_text)[:, 1]   \"1\" is case\n",
    "    print('---------------------this is classifier_3----------------------')\n",
    "    print('y_train_number_before_change_label',y_train.value_counts())\n",
    "\n",
    "    # first replace 1,2 with 0, then replace 3, with 1\n",
    "    y_train = y_train.replace(1, 0)\n",
    "    y_train = y_train.replace(2, 0)\n",
    "    y_test = y_test.replace(1, 0)\n",
    "    y_test = y_test.replace(2, 0)\n",
    "\n",
    "    # then replace 3 with 1,\n",
    "    y_train = y_train.replace(3, 1)\n",
    "    y_test = y_test.replace(3, 1)\n",
    "\n",
    "    # print('y_train', y_train)\n",
    "    print('y_train_number_after_change_label', y_train.value_counts())\n",
    "\n",
    "# not address imbalance\n",
    "#     X_train_imbalanced, y_train_imbalanced = X_train, y_train\n",
    "\n",
    "    # address imbalance\n",
    "    # cc = SMOTETomek(random_state=42,sampling_strategy='minority')\n",
    "    # cc = RandomOverSampler(random_state=42)\n",
    "    # cc = ClusterCentroids(random_state=42)\n",
    "\n",
    "    # X_train, y_train = cc.fit_resample(X_train, y_train)\n",
    "\n",
    "# GridSearchCV\n",
    "    classifier = RandomForestClassifier(random_state=seed)\n",
    "#     classifier = DecisionTreeClassifier(random_state=seed)\n",
    "    params = {\n",
    "               'n_estimators': [110],  # list(range(20,120,10)), for 6hours: [100], for 24hours, mean, [110]; for 24hours, worst, [110]\n",
    "              'max_depth': [4], # for 6hour: [8], for 24hours, mean, [8] # list(range(2,20,2)) # for 24 hours, worst, [6],[10]\n",
    "              # 'criterion': ['gini', 'entropy'], # 'gini', 'entropy'\n",
    "              #  'min_samples_split': [8], # list(range(2,10,2))\n",
    "              'class_weight': ['balanced'],  # , None # for 24 hours, worst, [balanced]\n",
    "              # 'max_leaf_nodes': list(range(2,20,2)),\n",
    "              # 'max_features': list(range(2,20,2)),\n",
    "              # 'min_samples_leaf': list(range(2,20,2))\n",
    "              }\n",
    "    grid = GridSearchCV(classifier, param_grid=params, scoring='roc_auc', cv=5, n_jobs=4) # scoring='roc_auc'\n",
    "    grid.fit(X_train, y_train)\n",
    "    scores = pd.DataFrame(grid.cv_results_)\n",
    "    # print('scores',scores)\n",
    "    grid_accs = scores['mean_test_score'].values.tolist()\n",
    "    best_auc = grid.best_score_\n",
    "    best_auc_std = scores.loc[grid.best_index_, 'std_test_score']\n",
    "    # print(best_auc_std)\n",
    "    classifier = grid.best_estimator_\n",
    "\n",
    "    print('Best by searching: %s, Std: %s' % (best_auc, best_auc_std))\n",
    "    best_train_auc= best_auc\n",
    "    print(grid.best_params_)\n",
    "\n",
    "    # load the classifier\n",
    "    # with open(file_path + 'ori_RW.pkl', 'rb') as fid:\n",
    "    #     classifier = pkl.load(fid)\n",
    "\n",
    "# training classifier\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    y_proba = classifier.predict_proba(X_test)[:, 1]\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    test_result = y_proba\n",
    "    # print('y_proba',y_proba)\n",
    "    aucroc = roc_auc_score(y_test, y_proba)\n",
    "    print('aucroc', aucroc)\n",
    "\n",
    "    # precision-recall auc\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "    # print('precision',precision)\n",
    "    # print('recall',recall)\n",
    "    # print('thresholds',thresholds)\n",
    "    aucpr = auc(recall, precision)\n",
    "    print('aucpr', aucpr)\n",
    "\n",
    "    precision_RW = precision_score(y_test, y_pred)\n",
    "    recall_RW = recall_score(y_test, y_pred)\n",
    "    confusion_matrix_RW = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print('precision_RW:', precision_RW)\n",
    "    print('recall_RW:', recall_RW)\n",
    "    print('confusion_matrix:', confusion_matrix_RW)\n",
    "\n",
    "# # plot precision recall curve\n",
    "    # fig, ax = plt.subplots()\n",
    "    # ax.plot(recall, precision, color='purple')\n",
    "    #\n",
    "    # # add axis labels to plot\n",
    "    # ax.set_title('RW-Precision-Recall Curve')\n",
    "    # ax.set_ylabel('Precision')\n",
    "    # ax.set_xlabel('Recall')\n",
    "    # plt.ylim(bottom=0)\n",
    "    # # display plot\n",
    "    # # plt.show()\n",
    "    # current_time = datetime.now()\n",
    "    # fig_name_0 = \"RW_Precision_Recall_curve\"+\"_\"+str(current_time)+\".png\"\n",
    "    # fig.savefig(file_path + fig_name_0)\n",
    "    # plt.close('all')\n",
    "\n",
    "    # save the classifier\n",
    "    with open(file_path + 'RW_ori.pkl', 'wb') as fid:\n",
    "        pkl.dump(classifier, fid)\n",
    "\n",
    "#  plot feature importance\n",
    "    X_data = X_train\n",
    "    feature_cols = feature_cols\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 12))\n",
    "    fig.subplots_adjust(left=0.2, right=0.95, top=0.95, bottom=0.08, hspace=.5, wspace=.15)\n",
    "\n",
    "    importances = classifier.feature_importances_\n",
    "    # print('importances',importances)\n",
    "    num_features = len(importances)\n",
    "    indices = np.argsort(abs(importances))[::-1] #\n",
    "    plt.title(\"Feature importance\")\n",
    "    plt.barh(list(range(X_data.shape[1]))[:num_features], list(importances[indices])[:num_features], align=\"center\")\n",
    "    plt.yticks(list(range(X_data.shape[1]))[:num_features], (np.array(feature_cols)[indices]).tolist()[:num_features])\n",
    "\n",
    "    # plt.show()\n",
    "    fig_name = file_path+classifier_flag+'_cv_fold_'+str(n_fold)+'_feature_importance_aucroc'+str(aucroc)+'_aucpr_'+str(aucpr)+'.png'\n",
    "    fig.savefig(fig_name)\n",
    "    plt.close('all')\n",
    "#\n",
    "#     # print(importances.shape)\n",
    "#     importances_df = pd.DataFrame(importances.reshape((1,-1)),columns=feature_cols)\n",
    "#     importances_df.to_csv(file_path+classifier_flag+'_cv_fold_'+str(n_fold)+'_feature_importance_aucroc'+str(aucroc)+'_aucpr_'+str(aucpr)+'.csv',index=False,header=True)\n",
    "#\n",
    "    if distill_method == \"mimic_teacher\":\n",
    "        if distill_model == \"tree\":\n",
    "            stu_model = DecisionTreeRegressor(max_depth=4)\n",
    "        elif distill_model == \"lr\":\n",
    "            # stu_model = LinearRegression()\n",
    "            stu_model = ElasticNet(random_state=seed,normalize=True) #LinearRegression()\n",
    "            # stu_model = LogisticRegression()\n",
    "            # tune\n",
    "            params = {'alpha':[0.00003], #[0.0003,0.1,0.01,0.0001,1,2] # 0.000003, 0.0003, 0.00025\n",
    "                'l1_ratio': [1],\n",
    "                    }\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        y_proba_train = classifier.predict_proba(X_train)[:, 1]\n",
    "        y_proba_test = classifier.predict_proba(X_test)[:, 1]\n",
    "        y_pos_proba_test = classifier.predict_proba(X_test.loc[y_test[y_test == 1].index])[:, 1]\n",
    "\n",
    "        grid = GridSearchCV(stu_model, param_grid=params, scoring='neg_mean_squared_error',  cv=5, n_jobs=4)  # scoring='roc_auc', 'precision'\n",
    "        grid.fit(X_train, y_proba_train)\n",
    "        stu_model = grid.best_estimator_\n",
    "        # print('Best by searching: %s, Std: %s' % (best_acc, best_acc_std))\n",
    "        print(grid.best_params_)\n",
    "\n",
    "        # # only select positive samples for testing\n",
    "        # positive_index = list(y_train[y_train == 1].index)\n",
    "        # X_train = X_train.loc[positive_index]\n",
    "        # positive_index_in_proba = np.argwhere(np.array(y_train) == 1).flatten()\n",
    "        # y_proba_train = y_proba_train[positive_index_in_proba]\n",
    "        # # build GridSearch\n",
    "        # model_name = 'elasticNet'\n",
    "        # best_alpha,best_l1_ratio,best_model,best_rmse = Build_GridSearch(model_name, params, X_train, y_proba_train, y_train) # y_train for selecting only positive samples\n",
    "        # # print('best model hyperparameters:',best_alpha, best_l1_ratio)\n",
    "        # # print('best model hyperparameters----best_rmse:', best_rmse)\n",
    "        # stu_model = best_model\n",
    "\n",
    "        # training classifier on all train data\n",
    "        stu_model.fit(X_train, y_proba_train)\n",
    "        print('stu_model.coef_',stu_model.coef_)\n",
    "\n",
    "        # save the stu_model\n",
    "        with open(file_path + 'RW_simplified.pkl', 'wb') as fid:\n",
    "            pkl.dump(stu_model, fid)\n",
    "\n",
    "    elif distill_method == \"train_from_scratch\":\n",
    "        if distill_model == \"tree\":\n",
    "            stu_model = DecisionTreeClassifier(max_depth=4)\n",
    "        elif distill_model == \"lr\":\n",
    "            stu_model = LogisticRegression()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        stu_model.fit(X_train, y_train)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    print(\"For Student Model:\")\n",
    "    y_proba_test_stu = stu_model.predict(X_test)\n",
    "\n",
    "    stu_model_rmse = mean_squared_error(y_proba_test, y_proba_test_stu, squared=False)\n",
    "    stu_model_mse = mean_squared_error(y_proba_test, y_proba_test_stu, squared=True)\n",
    "    print('stu_model_rmse', stu_model_rmse)\n",
    "    print('stu_model_mse', stu_model_mse)\n",
    "\n",
    "    # only focus on positive sample's rmse and mse\n",
    "    X_pos_test = X_test.loc[y_test[y_test == 1].index]\n",
    "    y_pos_proba_test_stu = stu_model.predict(X_pos_test)\n",
    "    stu_pos_model_rmse = mean_squared_error(y_pos_proba_test, y_pos_proba_test_stu, squared=False)\n",
    "    stu_pos_model_mse = mean_squared_error(y_pos_proba_test, y_pos_proba_test_stu, squared=True)\n",
    "    print('stu_pos_model_rmse', stu_pos_model_rmse)\n",
    "    print('stu_pos_model_mse', stu_pos_model_mse)\n",
    "\n",
    "# compute true positive\n",
    "    RW_threshold = 0.50  # 0.55\n",
    "    pre_pos = np.count_nonzero(y_pos_proba_test_stu >= 0.5) / len(y_pos_proba_test_stu)\n",
    "    # print('pre_pos', pre_pos)\n",
    "    y_pred_test_stu = y_proba_test_stu.copy()\n",
    "    y_pred_test_stu[y_pred_test_stu >= RW_threshold] = 1\n",
    "    y_pred_test_stu[y_pred_test_stu < RW_threshold] = 0\n",
    "\n",
    "    precision_RW_stu = precision_score(y_test, y_pred_test_stu)\n",
    "    print('precision_RW_stu:', precision_RW_stu)\n",
    "    recall_RW_stu = recall_score(y_test, y_pred_test_stu)\n",
    "    print('recall_RW_stu:', recall_RW_stu)\n",
    "    confusion_matrix_RW_stu = confusion_matrix(y_test, y_pred_test_stu)\n",
    "    print('confusion_matrix_stu:', confusion_matrix_RW_stu)\n",
    "\n",
    "    aucroc = roc_auc_score(y_test, y_proba_test_stu)\n",
    "    print('aucroc', aucroc)\n",
    "    # # precision-recall auc\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_proba_test_stu)\n",
    "    aucpr = auc(recall, precision)\n",
    "    print('aucpr', aucpr)\n",
    "\n",
    "    # # plot precision recall curve\n",
    "    # fig, ax = plt.subplots()\n",
    "    # ax.plot(recall, precision, color='purple')\n",
    "    #\n",
    "    # # add axis labels to plot\n",
    "    # ax.set_title('stu_RW-Precision-Recall Curve')\n",
    "    # ax.set_ylabel('Precision')\n",
    "    # ax.set_xlabel('Recall')\n",
    "    # plt.ylim(bottom=0)\n",
    "    # # display plot\n",
    "    # # plt.show()\n",
    "    # current_time = datetime.now()\n",
    "    # fig_name_0 = \"stu_RW_Precision_Recall_curve\" + \"_\" + str(current_time) + \".png\"\n",
    "    # fig.savefig(file_path + fig_name_0)\n",
    "    # plt.close('all')\n",
    "\n",
    "#  plot feature importance\n",
    "    X_data = X_train\n",
    "    feature_cols = feature_cols\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 12))\n",
    "    fig.subplots_adjust(left=0.2, right=0.95, top=0.95, bottom=0.08, hspace=.5, wspace=.15)\n",
    "\n",
    "    importances = stu_model.coef_\n",
    "    # importances = stu_model.coef_.reshape(-1)\n",
    "\n",
    "    # print('importances',importances)\n",
    "    num_features = len(importances)\n",
    "    indices = np.argsort(abs(importances))[::-1]\n",
    "    plt.title(\"Feature importance\")\n",
    "    plt.barh(list(range(X_data.shape[1]))[:num_features], list(importances[indices])[:num_features], align=\"center\")\n",
    "    plt.yticks(list(range(X_data.shape[1]))[:num_features], (np.array(feature_cols)[indices]).tolist()[:num_features])\n",
    "\n",
    "    # plt.show()\n",
    "    fig_name = file_path+classifier_flag+'_student_'+'.png'\n",
    "    fig.savefig(fig_name)\n",
    "    plt.close('all')\n",
    "\n",
    "    return test_result\n",
    "    # return y_proba_test_stu\n",
    "\n",
    "\n",
    "def compute_xAUC_RI(C_1_result, C_3_result, y_test, file_path):\n",
    "    RI_proba = []\n",
    "    RI_y_test = []\n",
    "    y_test = list(y_test)\n",
    "    for i in range(len(C_1_result)):\n",
    "        C_y_test = y_test[i]\n",
    "        if C_y_test != 3:  # keeping RI group\n",
    "            # if C_y_test != 1:  # keeping RW group\n",
    "            if C_y_test == 1:  # for RI group\n",
    "                # if C_y_test == 3: # for RW group\n",
    "                RI_y_test.append(1)\n",
    "                C_proba = C_1_result[i]\n",
    "                RI_proba.append(C_proba)\n",
    "            else:\n",
    "                RI_y_test.append(0)\n",
    "                C_proba = C_3_result[i]\n",
    "                RI_proba.append(C_proba)\n",
    "\n",
    "    y_proba = np.array(RI_proba)\n",
    "    y_test = np.array(RI_y_test)\n",
    "\n",
    "    aucroc = roc_auc_score(y_test, y_proba)\n",
    "    print('aucroc', aucroc)\n",
    "\n",
    "    # precision-recall auc\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "    # print('precision', precision)\n",
    "    # print('recall', recall)\n",
    "    aucpr = auc(recall, precision)\n",
    "\n",
    "    print('aucpr', aucpr)\n",
    "    # # plot precision recall curve\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(recall, precision, color='purple')\n",
    "\n",
    "    # add axis labels to plot\n",
    "    ax.set_title('RI-Precision-Recall Curve')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_xlabel('Recall')\n",
    "    plt.ylim(bottom=0)\n",
    "    # display plot\n",
    "    # plt.show()\n",
    "    current_time = datetime.now()\n",
    "    fig_name_0 = \"xAUC\" + \"_RI_\" + str(current_time) + \".png\"\n",
    "    fig.savefig(file_path + fig_name_0)\n",
    "    plt.close('all')\n",
    "\n",
    "    return aucroc, aucpr\n",
    "\n",
    "def compute_xAUC_RW(C_1_result, C_3_result, y_test, file_path):\n",
    "    RI_proba = []\n",
    "    RI_y_test = []\n",
    "    y_test = list(y_test)\n",
    "    for i in range(len(C_1_result)):\n",
    "        C_y_test = y_test[i]\n",
    "        # if C_y_test != 3:  # keeping RI group\n",
    "        if C_y_test != 1:  # keeping RW group\n",
    "            # if C_y_test == 1:  # for RI group\n",
    "            if C_y_test == 3:  # for RW group\n",
    "                RI_y_test.append(1)\n",
    "                C_proba = C_1_result[i]\n",
    "                RI_proba.append(C_proba)\n",
    "            else:\n",
    "                RI_y_test.append(0)\n",
    "                C_proba = C_3_result[i]\n",
    "                RI_proba.append(C_proba)\n",
    "\n",
    "    y_proba = np.array(RI_proba)\n",
    "    y_test = np.array(RI_y_test)\n",
    "\n",
    "    aucroc = roc_auc_score(y_test, y_proba)\n",
    "    print('aucroc', aucroc)\n",
    "\n",
    "    # precision-recall auc\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "    # print('precision', precision)\n",
    "    # print('recall', recall)\n",
    "    aucpr = auc(recall, precision)\n",
    "\n",
    "    print('aucpr', aucpr)\n",
    "    # # plot precision recall curve\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(recall, precision, color='purple')\n",
    "\n",
    "    # add axis labels to plot\n",
    "    ax.set_title('RW-Precision-Recall Curve')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_xlabel('Recall')\n",
    "    plt.ylim(bottom=0)\n",
    "    # display plot\n",
    "    # plt.show()\n",
    "    current_time = datetime.now()\n",
    "    fig_name_0 = \"xAUC\" + \"_RW_\" + str(current_time) + \".png\"\n",
    "    fig.savefig(file_path + fig_name_0)\n",
    "    plt.close('all')\n",
    "    return aucroc, aucpr\n",
    "\n",
    "\n",
    "def change_label(data_df, case_flag):\n",
    "    data_df = data_df\n",
    "\n",
    "    if case_flag == 0:\n",
    "        # first change case label 0->4;\n",
    "        data_df.loc[data_df['group'] == 0, 'group'] = 4\n",
    "\n",
    "        # then change control lable 1->0, 2->0, 3->0\n",
    "        data_df.loc[data_df['group'] == 1, 'group'] = 0\n",
    "        data_df.loc[data_df['group'] == 2, 'group'] = 0\n",
    "        data_df.loc[data_df['group'] == 3, 'group'] = 0\n",
    "\n",
    "        # third change case label 4->1;  \"1\" is fouced.\n",
    "        data_df.loc[data_df['group'] == 4, 'group'] = 1\n",
    "\n",
    "    if case_flag == 1:\n",
    "        # change label, 0->0, 2->0, 3->0; ------1 vs rest\n",
    "        data_df.loc[data_df['group'] == 0, 'group'] = 0\n",
    "        data_df.loc[data_df['group'] == 2, 'group'] = 0\n",
    "        data_df.loc[data_df['group'] == 3, 'group'] = 0\n",
    "    if case_flag == 2:\n",
    "        # first change control lable 0->0, 1->0, 3->0\n",
    "        data_df.loc[data_df['group'] == 0, 'group'] = 0\n",
    "        data_df.loc[data_df['group'] == 1, 'group'] = 0\n",
    "        data_df.loc[data_df['group'] == 3, 'group'] = 0\n",
    "\n",
    "        # then change case label 2->1;  \"1\" is fouced.\n",
    "        data_df.loc[data_df['group'] == 2, 'group'] = 1\n",
    "\n",
    "    if case_flag == 3:\n",
    "        # first change control lable 0->0, 1->0, 2->0\n",
    "        data_df.loc[data_df['group'] == 0, 'group'] = 0\n",
    "        data_df.loc[data_df['group'] == 1, 'group'] = 0\n",
    "        data_df.loc[data_df['group'] == 2, 'group'] = 0\n",
    "\n",
    "        # then change case label 3->1;  \"1\" is fouced.\n",
    "        data_df.loc[data_df['group'] == 3, 'group'] = 1\n",
    "\n",
    "    return data_df\n",
    "\n",
    "def clean_outliers(df,cols):\n",
    "    # cols: list\n",
    "    for col in cols:\n",
    "        min_value = df[col].quantile(0.01)\n",
    "        max_value = df[col].quantile(0.99)\n",
    "        df[col][df[col] < min_value] = np.nan\n",
    "        df[col][df[col] > max_value] = np.nan\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_data_mimic = 'Subtyping/MIMIC-IV/data/'\n",
    "file_path_data_eicu = 'Subtyping/eICU/data/'\n",
    "file_path = 'Subtyping/models/'\n",
    "\n",
    "dataset_flag = 'combined'# MIMIC, eICU, 'combined','M_train_e_test'# train on MIMIC, test on\n",
    "two_set_flag = 0\n",
    "\n",
    "num_sel_feat = -1 # -1 to keep all the features\n",
    "# \"train_from_scratch\" or \"mimic_teacher\"\n",
    "distill_method = \"train_from_scratch\" # \"train_from_scratch\"\n",
    "# \"tree\" or \"lr\"\n",
    "distill_model = \"lr\"\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "prediction_feature = 'prediction_feature_6'\n",
    "\n",
    "# read data\n",
    "data_df_ori_1 = pd.read_csv(file_path_data_mimic + 'features_24h.csv',dtype={'patientunitstayid':str}, index_col=0) # Note that, using patientunitstayid as index, MIMIC: data_df_ori  24 hours data\n",
    "\n",
    "data_df_ori_2 = pd.read_csv(file_path_data_eicu + 'features_24h.csv', dtype={'patientunitstayid':str}, index_col=0) # Note that, using patientunitstayid as index, eICU: data_df_ori, 24 hours\n",
    "\n",
    "print('len(data_df_ori_1)',len(data_df_ori_1))\n",
    "print('len(data_df_ori_2)',len(data_df_ori_2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# remove outlier values\n",
    "outlier_cols = ['Bands', 'CRP', 'Temperature', 'WBC', 'SO2', 'Respiratory_rate',\n",
    "             'Heart_rate', 'Lactate', 'Systolic_ABP', 'BUN', 'Creatinine',\n",
    "            'ALT', 'AST', 'Bilirubin','GCS','Hemoglobin', 'INR', 'Platelet','Albumin',\n",
    "            'Chloride', 'Glucose', 'Sodium', 'BMI'] # no 'Age', 'Comorbidity_score', 'Pao2',\n",
    "\n",
    "data_df_ori_1 = clean_outliers(data_df_ori_1, outlier_cols)\n",
    "data_df_ori_2 = clean_outliers(data_df_ori_2, outlier_cols)\n",
    "\n",
    "data_df_ori_1.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "data_df_ori_2.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "data_df_1 = data_df_ori_1.fillna(data_df_ori_1.median())\n",
    "data_df_2 = data_df_ori_2.fillna(data_df_ori_2.median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "drug_variable = ['Dopamine_total', 'Dobutamine_total', 'Norepinephrine_total', 'Epinephrine_total', 'Phenylephrine_total', 'Vasopressin_total','MAP', 'MAP_NEQ','Total_DDNEP']\n",
    "# drug_variable = ['MAP_NEQ','Total_DDNEP']\n",
    "\n",
    "obj_feature_name = ['SOFA_score', 'Respiration_score', 'Coagulation_score', 'Liver_score',\n",
    "                     'Cardiovascular_score','CNS_score', 'Renal_score',\n",
    "             'CRP', 'Temperature', 'WBC', 'SO2', 'Pao2', 'Respiratory_rate',\n",
    "            'Heart_rate', 'Lactate', 'Systolic_ABP','BUN', 'Creatinine',\n",
    "            'ALT', 'AST', 'Bilirubin','GCS','Hemoglobin', 'INR', 'Platelet',\n",
    "            'Chloride', 'Glucose', 'Sodium', 'BMI',\n",
    "            'Age']\n",
    "\n",
    "\n",
    "if dataset_flag =='eICU':\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_df_2[obj_feature_name], data_df_2['group'], test_size=0.2, random_state=seed)\n",
    "\n",
    "elif dataset_flag =='MIMIC':\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_df_1[obj_feature_name], data_df_1['group'], test_size=0.2, random_state=seed)\n",
    "\n",
    "elif dataset_flag == 'M_train_e_test': # train on MIMIC and testing on eICU\n",
    "    X_train = data_df_1[obj_feature_name]\n",
    "    X_test = data_df_2[obj_feature_name]\n",
    "    y_train = data_df_1['group']\n",
    "    y_test = data_df_2['group']\n",
    "else:\n",
    "    two_set_flag = 1\n",
    "    X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(data_df_1[obj_feature_name], data_df_1['group'], test_size=0.2, random_state=seed)\n",
    "    X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(data_df_2[obj_feature_name], data_df_2['group'], test_size=0.2, random_state=seed)\n",
    "\n",
    "    X_train = pd.concat([X_train_1, X_train_2], axis=0, ignore_index=True) # , ignore_index=True\n",
    "    y_train = pd.concat([y_train_1, y_train_2], axis=0, ignore_index=True) # , ignore_index=True\n",
    "\n",
    "    X_test = pd.concat([X_test_1, X_test_2], axis=0, ignore_index=True) # , ignore_index=True\n",
    "    y_test = pd.concat([y_test_1, y_test_2], axis=0, ignore_index=True) # , ignore_index=True\n",
    "\n",
    "X_ori = pd.concat([X_train, X_test], axis=0, ignore_index=True)\n",
    "y_ori = pd.concat([y_train, y_test], axis=0, ignore_index=True)\n",
    "print('len X:', len(X_train))\n",
    "print('len y:', len(y_train))\n",
    "\n",
    "feature_cols = obj_feature_name\n",
    "n_fold = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(X_train,y_train,X_test,y_test,feature_cols,classifier_flag, n_fold,file_path,group): #X[train], y[train],X[test]\n",
    "# change label    so as to the objective case is \"1\", the objective of change label is to keep  classifier.predict_proba(X_text)[:, 1]   \"1\" is case\n",
    "    print('---------------------this is classifier_1----------------------')\n",
    "    print('y_train_number_before_change_label',y_train.value_counts())\n",
    "    # replace 2,3 with 0,\n",
    "    if group == 1:\n",
    "        save_label = 'RI'\n",
    "        y_train = y_train.replace(2, 0)\n",
    "        y_train = y_train.replace(3, 0)\n",
    "        y_test = y_test.replace(2, 0)\n",
    "        y_test = y_test.replace(3, 0)\n",
    "    if group == 3:\n",
    "        save_label = 'RW'\n",
    "        y_train = y_train.replace(1, 0)\n",
    "        y_train = y_train.replace(2, 0)\n",
    "        y_test = y_test.replace(1, 0)\n",
    "        y_test = y_test.replace(2, 0)\n",
    "\n",
    "        y_train = y_train.replace(3, 1)\n",
    "        y_test = y_test.replace(3, 1)\n",
    "\n",
    "\n",
    "    # print('y_train', y_train)\n",
    "    print('y_train_number_after_change_label', y_train.value_counts())\n",
    "\n",
    "# GridSearchCV\n",
    "    classifier = RandomForestClassifier(random_state=seed)\n",
    "\n",
    "    params = {\n",
    "              'n_estimators': [110], # for 24hours, worst, [110]\n",
    "              'max_depth': [10], # for 24hours, mean, [10],for 24hours, worst, [12]\n",
    "              'criterion': ['gini'],\n",
    "               'min_samples_split': [2],\n",
    "              'class_weight': ['balanced'],  # , None\n",
    "              'max_leaf_nodes': [8],\n",
    "              'max_features': [2],\n",
    "              'min_samples_leaf': [12]\n",
    "              }\n",
    "\n",
    "    grid = GridSearchCV(classifier, param_grid=params, scoring='roc_auc', cv=5, n_jobs=4) # scoring='roc_auc', 'precision'\n",
    "    grid.fit(X_train, y_train)\n",
    "    scores = pd.DataFrame(grid.cv_results_)\n",
    "    # print('scores',scores)\n",
    "    grid_accs = scores['mean_test_score'].values.tolist()\n",
    "    best_acc = grid.best_score_\n",
    "    best_acc_std = scores.loc[grid.best_index_, 'std_test_score']\n",
    "    # print(best_auc_std)\n",
    "    classifier = grid.best_estimator_\n",
    "    print('Best by searching: %s, Std: %s' % (best_acc, best_acc_std))\n",
    "    print(grid.best_params_)\n",
    "\n",
    "# training classifier on all train data\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_proba = classifier.predict_proba(X_test)[:, 1]\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    test_result = y_proba\n",
    "\n",
    "    aucroc = roc_auc_score(y_test, y_proba)\n",
    "    print('aucroc', aucroc)\n",
    "\n",
    "    # precision-recall auc\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "    aucpr = auc(recall, precision)\n",
    "    print('aucpr', aucpr)\n",
    "\n",
    "    precision_RI = precision_score(y_test, y_pred)\n",
    "    recall_RI = recall_score(y_test, y_pred)\n",
    "    confusion_matrix_RI = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print('precision_RI:', precision_RI)\n",
    "    print('recall_RI:', recall_RI)\n",
    "    print('confusion_matrix_RI:', confusion_matrix_RI)\n",
    "\n",
    "    # # plot precision recall curve\n",
    "    # fig, ax = plt.subplots()\n",
    "    # ax.plot(recall, precision, color='purple')\n",
    "    #\n",
    "    # #add axis labels to plot\n",
    "    # ax.set_title('RI-Precision-Recall Curve')\n",
    "    # ax.set_ylabel('Precision')\n",
    "    # ax.set_xlabel('Recall')\n",
    "    # plt.ylim(bottom=0)\n",
    "    # #display plot\n",
    "    # # plt.show()\n",
    "    # current_time = datetime.now()\n",
    "    # fig_name_0 = \"RI_Precision_Recall_curve\"+\"_\"+str(current_time)+\".png\"\n",
    "    # fig.savefig(file_path + fig_name_0)\n",
    "    # plt.close('all')\n",
    "\n",
    "# plot decision tree\n",
    "#     fig = plt.figure(figsize=(50, 30))\n",
    "#     _ = tree.plot_tree(classifier,\n",
    "#                        feature_names=feature_cols,\n",
    "#                        class_names=[\"Not Ripidly Improving\", \"Ripidly Improving\"],\n",
    "#                        filled=True, fontsize=20)\n",
    "#\n",
    "#     fig_name = \"RI_visualization.png\"\n",
    "#     fig.savefig(file_path + fig_name)\n",
    "#     plt.close('all')\n",
    "\n",
    "# # save the classifier\n",
    "#     with open(file_path + f'{save_label}_ori.pkl', 'wb') as fid:\n",
    "#         pkl.dump(classifier, fid)\n",
    "\n",
    "#  plot feature importance\n",
    "    X_data = X_train\n",
    "    feature_cols = feature_cols\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 12))\n",
    "    fig.subplots_adjust(left=0.2, right=0.95, top=0.95, bottom=0.08, hspace=.5, wspace=.15)\n",
    "\n",
    "    importances = classifier.feature_importances_\n",
    "    # importances = classifier.coef_.reshape(-1)\n",
    "\n",
    "    # print('importances',importances)\n",
    "    num_features = len(importances)\n",
    "    indices = np.argsort(abs(importances))[::-1] #\n",
    "    plt.title(\"Feature importance\")\n",
    "    plt.barh(list(range(X_data.shape[1]))[:num_features], list(importances[indices])[:num_features], align=\"center\")\n",
    "    plt.yticks(list(range(X_data.shape[1]))[:num_features], (np.array(feature_cols)[indices]).tolist()[:num_features])\n",
    "\n",
    "    # plt.show()\n",
    "    fig_name = file_path+classifier_flag+'_cv_fold_'+str(n_fold)+'_feature_importance_aucroc'+str(aucroc)+'_aucpr_'+str(aucpr)+'.png'\n",
    "    fig.savefig(fig_name, facecolor='white', transparent=False)\n",
    "    plt.close('all')\n",
    "\n",
    "    # print(importances.shape)\n",
    "    importances_df = pd.DataFrame(importances.reshape((1,-1)),columns=feature_cols)\n",
    "    importances_df.to_csv(file_path+classifier_flag+'_teacher'+'.csv',index=False,header=True)\n",
    "\n",
    "    if num_sel_feat > 0:\n",
    "        sel_idx = np.argsort(importances)[::-1][:num_sel_feat]\n",
    "        X_train = X_train.iloc[:, sel_idx]\n",
    "        X_test = X_test.iloc[:, sel_idx]\n",
    "\n",
    "    y_proba_train = classifier.predict_proba(X_train)[:, 1]\n",
    "    y_proba_test = classifier.predict_proba(X_test)[:, 1]\n",
    "    y_pos_proba_test = classifier.predict_proba(X_test.loc[y_test[y_test == 1].index])[:, 1]\n",
    "\n",
    "    if distill_method == \"mimic_teacher\":\n",
    "        if distill_model == \"tree\":\n",
    "            stu_model = DecisionTreeRegressor(max_depth=4)\n",
    "        elif distill_model == \"lr\":\n",
    "            stu_model = ElasticNet(random_state=seed,normalize=True) #LinearRegression()\n",
    "            # stu_model = LogisticRegression()\n",
    "            # tune\n",
    "            params = {'alpha':[0.00003], # 0.0003, 0.00025,0.0004, 0.00005,0.00045, 0.01\n",
    "                    'l1_ratio': [1],\n",
    "                    }\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        grid = GridSearchCV(stu_model, param_grid=params, scoring='neg_mean_squared_error',  cv=5, n_jobs=4)  # scoring='roc_auc', 'precision'\n",
    "        grid.fit(X_train, y_proba_train)\n",
    "        stu_model = grid.best_estimator_\n",
    "        print('Best by searching: %s, Std: %s' % (best_acc, best_acc_std))\n",
    "        print(grid.best_params_)\n",
    "\n",
    "        # # build GridSearch\n",
    "        # model_name = 'elasticNet'\n",
    "        # # only select positive samples\n",
    "        # positive_index = list(y_train[y_train == 1].index)\n",
    "        # X_train = X_train.loc[positive_index]\n",
    "        # positive_index_in_proba = np.argwhere(np.array(y_train) == 1).flatten()\n",
    "        # y_proba_train = y_proba_train[positive_index_in_proba]\n",
    "        #\n",
    "        # best_alpha,best_l1_ratio,best_model,best_rmse = Build_GridSearch(model_name, params, X_train, y_proba_train, y_train) # y_train for selecting only positive samples\n",
    "        # print('best model hyperparameters:',best_alpha, best_l1_ratio)\n",
    "        # print('best model hyperparameters----best_rmse:', best_rmse)\n",
    "        # stu_model = best_model\n",
    "\n",
    "        # training classifier on all train data\n",
    "        stu_model.fit(X_train, y_proba_train)\n",
    "        print('stu_model.coef_',stu_model.coef_)\n",
    "\n",
    "        # # save the stu_model\n",
    "        # with open(file_path + f'{save_label}_simplified.pkl', 'wb') as fid:\n",
    "        #     pkl.dump(stu_model, fid)\n",
    "\n",
    "        # with open(file_path+'RI_simplified.pkl', 'rb') as file:\n",
    "        #     dataee = pickle.load(file)\n",
    "        # print(dataee)\n",
    "\n",
    "    elif distill_method == \"train_from_scratch\":\n",
    "        if distill_model == \"tree\":\n",
    "            stu_model = DecisionTreeClassifier(max_depth=4)\n",
    "        elif distill_model == \"lr\":\n",
    "            stu_model = sklearn.pipeline.Pipeline([\n",
    "                ('scale', preprocessing.StandardScaler()),\n",
    "                ('model', LogisticRegression(class_weight='balanced', penalty='elasticnet', solver='saga', l1_ratio=0.5))\n",
    "            ])\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        stu_model.fit(X_train, y_train)\n",
    "        # save the stu_model\n",
    "        with open(file_path + f'{save_label}_simplified.pkl', 'wb') as fid:\n",
    "            pkl.dump(stu_model, fid)\n",
    "    \n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    print(\"For Student Model:\")\n",
    "    y_proba_test_stu = stu_model.predict(X_test) #\n",
    "\n",
    "    stu_model_rmse = mean_squared_error(y_proba_test, y_proba_test_stu, squared=False)\n",
    "    stu_model_mse = mean_squared_error(y_proba_test, y_proba_test_stu, squared=True)\n",
    "    print('stu_model_rmse',stu_model_rmse)\n",
    "    print('stu_model_mse',stu_model_mse)\n",
    "\n",
    "    # only focus on positive sample's rmse and mse\n",
    "    X_pos_test = X_test.loc[y_test[y_test == 1].index]\n",
    "    y_pos_proba_test_stu = stu_model.predict(X_pos_test)\n",
    "    stu_pos_model_rmse = mean_squared_error(y_pos_proba_test, y_pos_proba_test_stu, squared=False)\n",
    "    stu_pos_model_mse = mean_squared_error(y_pos_proba_test, y_pos_proba_test_stu, squared=True)\n",
    "    print('stu_pos_model_rmse', stu_pos_model_rmse)\n",
    "    print('stu_pos_model_mse', stu_pos_model_mse)\n",
    "\n",
    "    # compute true positive\n",
    "    RI_threshold = 0.5 # 0.65\n",
    "    rec_pos = np.count_nonzero(y_pos_proba_test_stu >= RI_threshold)/len(y_pos_proba_test_stu)\n",
    "    print('rec_pos',rec_pos)\n",
    "    y_pred_test_stu = y_proba_test_stu.copy()\n",
    "    y_pred_test_stu[y_pred_test_stu>=RI_threshold] = 1\n",
    "    y_pred_test_stu[y_pred_test_stu<RI_threshold] = 0\n",
    "\n",
    "    precision_RI_stu = precision_score(y_test, y_pred_test_stu)\n",
    "    print('precision_RI_stu:', precision_RI_stu)\n",
    "    recall_RI_stu = recall_score(y_test, y_pred_test_stu)\n",
    "    print('recall_RI_stu:', recall_RI_stu)\n",
    "    confusion_matrix_RI_stu = confusion_matrix(y_test, y_pred_test_stu)\n",
    "    print('confusion_matrix_stu:', confusion_matrix_RI_stu)\n",
    "\n",
    "    aucroc = roc_auc_score(y_test, y_proba_test_stu)\n",
    "    print('aucroc', aucroc)\n",
    "\n",
    "    # precision-recall auc\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_proba_test_stu)\n",
    "    aucpr = auc(recall, precision)\n",
    "    print('aucpr', aucpr)\n",
    "\n",
    "\n",
    "    # # plot precision recall curve\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(recall, precision, color='purple')\n",
    "    \n",
    "    #add axis labels to plot\n",
    "    ax.set_title(f'{save_label} Precision-Recall Curve')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_xlabel('Recall')\n",
    "    plt.ylim(bottom=0)\n",
    "    #display plot\n",
    "    # plt.show()\n",
    "    current_time = datetime.now()\n",
    "    fig_name_0 = f\"{save_label}_Precision_Recall_curve\"+\"_\"+str(current_time)+\".png\"\n",
    "    fig.savefig(file_path + fig_name_0)\n",
    "    plt.close('all')\n",
    "\n",
    "#  plot feature importance\n",
    "    X_data = X_train\n",
    "    feature_cols = feature_cols\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 12))\n",
    "    fig.subplots_adjust(left=0.2, right=0.95, top=0.95, bottom=0.08, hspace=.5, wspace=.15)\n",
    "\n",
    "    if distill_method == \"train_from_scratch\":\n",
    "        importances = stu_model.named_steps['model'].coef_\n",
    "    else:\n",
    "        importances = stu_model.coef_\n",
    "    \n",
    "    if len(importances) == 1:\n",
    "        importances = importances.reshape(-1)\n",
    "\n",
    "    # print('importances',importances)\n",
    "    num_features = len(importances)\n",
    "    indices = np.argsort(abs(importances))[::-1] #\n",
    "    plt.title(\"Feature importance\")\n",
    "    plt.barh(list(range(X_data.shape[1]))[:num_features], list(importances[indices])[:num_features], align=\"center\")\n",
    "    plt.yticks(list(range(X_data.shape[1]))[:num_features], (np.array(feature_cols)[indices]).tolist()[:num_features])\n",
    "\n",
    "    # # plt.show()\n",
    "    # fig_name = file_path+classifier_flag+'_student_'+'.png'\n",
    "    # fig.savefig(fig_name, facecolor='white', transparent=False)\n",
    "    # plt.close('all')\n",
    "\n",
    "    return test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_1_result = build_classifier(X_train, y_train, X_test, y_test, feature_cols, 'classifier_1', n_fold, file_path, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_3_result = build_classifier(X_train, y_train, X_test, y_test, feature_cols, 'classifier_3', n_fold, file_path, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
